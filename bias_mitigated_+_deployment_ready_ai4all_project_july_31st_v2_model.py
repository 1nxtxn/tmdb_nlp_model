# -*- coding: utf-8 -*-
"""BIAS MITIGATED + DEPLOYMENT READY - AI4ALL Project - July 31st v2 model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AX_D-t2EQPEqcVvEZaTnNRCT5hMpRnmd

Action Items:
1. Accounting for BIAS: Maybe take what we said in the document/slides to counteract the bias and then implement it. Such as taking from the TMDb API and adding more movies to the dataset with different languages/countries of origin. Also make sure the reccommendation is randomizing the movies on release year and is not biased towards earlier release years.
3. Getting a quantification of the tone indicators for each cluster
4. Figuring out maybe a better K-means
5. Get a heirarchal clustering approach or visualization
6. (Optional) Experiment outside of using vader
"""

!pip install kagglehub[pandas-datasets]
!pip install vaderSentiment
!pip install scikit-learn
!pip install qgrid

"""# Section 2: Installing Necessary Dependencies and Loading Dataset"""

# Install dependencies as needed:
import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
pd.options.mode.chained_assignment = None
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import KNNImputer
import warnings
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA




file_path = "tmdb_5000_movies.csv"

#Loading the 5000 TMDb database
df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,
                                "tmdb/tmdb-movie-metadata",
                                file_path)

"""# Section 3: Feature Selection"""

# === Section 3: Feature Selection (with revenue included) ===

# 1) Remove exact-title duplicates
df.drop_duplicates(subset=['original_title'], keep='first', inplace=True)

# 2) Ensure revenue column exists and fill missing values
df['revenue'] = df.get('revenue', 0).fillna(0).astype(float)

# 3) Keep only the columns we need (now including revenue)
df = df[[
    'id',
    'original_title',
    'overview',
    'genres',
    'keywords',
    'production_countries',
    'release_date',
    'vote_average',
    'revenue'
]]

# 4) Drop any movies without an overview
df.dropna(subset=['overview'], inplace=True)

# 5) Fill missing keywords with an empty list
df['keywords'] = df['keywords'].fillna('[]')

print("Columns after feature selection:", df.columns.tolist())
print("Sample revenue for first record:", df.loc[0, 'revenue'])

"""# Section 4: Preprocessing and vectorizing the overviews"""

# === Section 4: Preprocessing and Vectorizing the Overviews ===

from sklearn.feature_extraction.text import TfidfVectorizer
import ast
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Tokenize overview
df['tokenized_overview'] = df['overview'].apply(word_tokenize)

# Preprocess function
def preprocess(words):
    words = [w.lower() for w in words if w.isalpha() or any(ch.isdigit() for ch in w)]
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    return words

# Apply preprocessing
df['clean_overview'] = df['tokenized_overview'].apply(preprocess).str.join(' ')

# Extract keywords as string
def extract_keywords(kw_list):
    try:
        parsed = ast.literal_eval(kw_list)
        return ' '.join([kw['name'] for kw in parsed])
    except:
        return ''

df['keywords_text'] = df['keywords'].apply(extract_keywords)

# Build semantic input: title + keywords + overview
df['text_input'] = df['original_title'] + ' ' + df['keywords_text'] + ' ' + df['clean_overview']

# ðŸ”§ Fix index alignment before vectorizing
df.reset_index(drop=True, inplace=True)

# TF-IDF vectorization on the new text_input
vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.8, sublinear_tf=True, max_features=30000)
X = vectorizer.fit_transform(df['text_input'])

print("TF-IDF matrix shape (semantic):", X.shape)

"""# Section 5: SENTIMENT ANALYSIS: Initializing VADER"""

# Function to extract sentiment score (compound value)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    return analyzer.polarity_scores(text)['compound']

# Apply to clean overviews
df['sentiment_score'] = df['overview'].apply(get_sentiment)

"""# Section 6: Classify tone as a label"""

# === Section 6: Sentiment Tone Classification ===

def classify_tone(score):
    if score >= 0.3:
        return 'positive'
    elif score <= -0.3:
        return 'negative'
    else:
        return 'neutral'

df['tone'] = df['sentiment_score'].apply(classify_tone)


# positive: The overview sounds optimistic, happy, uplifting, or hopeful.
# For example, it talks about success, friendship, love, or inspiring journeys.
# neutral: The overview is factual, descriptive, or informational without a clear emotional tone.
# For example, a straightforward plot summary with no strong emotion.
# negative: The overview conveys sadness, conflict, danger, tragedy, or generally a pessimistic or dark mood.

"""Preview"""

df[['original_title', 'sentiment_score', 'tone']].head(10)
df.size
df.value_counts('tone')

"""# Saving the vectorizer

---
## ðŸŽ¯ Loading saved TF-IDF artifacts

After youâ€™ve run the mounting/downloading steps and have the files in your Colab VM (or Drive), use:


### Load the fitted vectorizer
vectorizer = joblib.load("tfidf_vectorizer.joblib")

### Load the TF-IDF matrix
X = sparse.load_npz("tfidf_matrix.npz")
"""

# import joblib
# from scipy import sparse
# from google.colab import drive

# drive.mount('/content/drive')

# joblib.dump(vectorizer, "/content/drive/MyDrive/tfidf_vectorizer.joblib")
# sparse.save_npz("/content/drive/MyDrive/tfidf_matrix.npz", X)

"""# Section 9: Clustering and Recommendation Logic

### Optimal K-Means (Visualization)
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np

inertia = []
silhouette = []
# K_range = range(3, 57)
K_range = range(45, 54)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)
    silhouette.append(silhouette_score(X, kmeans.labels_))

# Elbow + silhouette plot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(K_range, inertia, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')

plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette, 'go-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method')
plt.tight_layout()
plt.show()

# Final clustering
optimal_k = K_range[np.argmax(silhouette)]
print(f"Optimal k: {optimal_k}")

kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df["cluster"] = kmeans.fit_predict(X)

print(df["cluster"].value_counts())

"""# Recommendation Logic (Fixed Error: Movies older than 2000 are given recommendations properly)"""

from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import ast
from collections import Counter
from IPython.display import display

# === Threshold for studio size ===
SMALL_THRESHOLD = 20_000_000  # $20M cutoff

# === Region groups mapping ===
REGION_GROUPS = {
    "North America": ["United States", "Canada", "Mexico"],
    "South America": ["Brazil", "Argentina", "Colombia", "Chile", "Peru", "Venezuela", "Ecuador", "Bolivia", "Paraguay", "Uruguay"],
    "Europe": ["United Kingdom", "France", "Germany", "Italy", "Spain", "Netherlands", "Sweden", "Norway", "Denmark", "Finland", "Belgium", "Switzerland", "Austria", "Poland", "Russia"],
    "Asia": ["Japan", "China", "India", "South Korea", "Hong Kong", "Taiwan", "Thailand", "Singapore", "Malaysia", "Indonesia"],
    "Africa": ["Nigeria", "Egypt", "South Africa", "Kenya", "Morocco", "Algeria", "Ghana", "Ethiopia"],
    "Oceania": ["Australia", "New Zealand", "Fiji"],
    "Latin America": ["Brazil", "Argentina", "Colombia", "Mexico", "Chile", "Peru", "Venezuela", "Ecuador"]
}

# === Helper functions ===

def show_example_matches(user_text):
    user_vec = vectorizer.transform([user_text])
    sims     = cosine_similarity(user_vec, X).flatten()
    df_preview = df.copy()
    df_preview["similarity"] = sims
    df_preview['release_year'] = pd.to_datetime(
        df_preview['release_date'], errors='coerce'
    ).dt.year.fillna(0).astype(int)
    preview_top = df_preview.sort_values("similarity", ascending=False).head(5)

    print("\nðŸ” Based on your input, here are some movie matches:")
    for _, row in preview_top.iterrows():
        year = row['release_year'] if row['release_year'] > 0 else "Unknown"
        print(f"- {row['original_title']} ({year})")

def print_genre_suggestions():
    all_genres = []
    for row in df['genres']:
        try:
            genres = ast.literal_eval(row)
            all_genres.extend([g['name'] for g in genres])
        except:
            continue
    top_genres = Counter(all_genres).most_common(10)
    print("\nðŸŽ¬ Example genres you can type:")
    print(", ".join([g for g, _ in top_genres]))

def print_region_suggestions():
    print("\nðŸ—ºï¸ Example regions you can type:")
    options = list(REGION_GROUPS.keys()) + ["Any"]
    print(", ".join(options))

# === Recommendation Function (exclude zero revenue, return studio_size only) ===

def recommend_movies(user_text, user_genre, start_year, end_year, movie_region="Any"):
    user_vec = vectorizer.transform([user_text])

    # Genre filtering
    if user_genre.lower() != "any":
        genre_mask = df['genres'].apply(
            lambda r: any(g['name'].lower() == user_genre.lower() for g in ast.literal_eval(r))
        )
    else:
        genre_mask = pd.Series(True, index=df.index)

    # Region filtering
    region_key = movie_region.strip().lower()
    if region_key in (k.lower() for k in REGION_GROUPS):
        key = next(k for k in REGION_GROUPS if k.lower() == region_key)
        allowed = {c.lower() for c in REGION_GROUPS[key]}
        region_mask = df['production_countries'].apply(
            lambda r: any(c['name'].lower() in allowed for c in ast.literal_eval(r))
        )
    elif movie_region.lower() != "any":
        region_mask = df['production_countries'].apply(
            lambda r: any(c['name'].lower() == region_key for c in ast.literal_eval(r))
        )
    else:
        region_mask = pd.Series(True, index=df.index)

    # Year filtering
    df['release_year'] = pd.to_datetime(df['release_date'], errors='coerce') \
                             .dt.year.fillna(0).astype(int)
    year_mask = df['release_year'].between(start_year, end_year)

    # Candidate subset & drop zero-revenue
    subset = df[genre_mask & region_mask & year_mask].copy()
    subset = subset[subset['revenue'] > 0]

    if subset.empty:
        print("âš ï¸ No movies with revenue data match your filters.")
        return pd.DataFrame(columns=[
            "original_title", "overview", "similarity", "release_year", "studio_size"
        ])

    # Compute similarity
    sims = cosine_similarity(user_vec, X[subset.index]).flatten()
    max_sim = sims.max()
    subset["similarity"] = sims / max_sim if max_sim > 0 else sims

    # Exclude zero-sim if possible
    nonzero = subset[subset["similarity"] > 0]
    working = nonzero if not nonzero.empty else subset

    # Pick top 3 and reset index
    top3 = working.sort_values("similarity", ascending=False).head(3).copy()
    top3.reset_index(drop=True, inplace=True)

    # Guarantee one small film
    if not (top3['revenue'] < SMALL_THRESHOLD).any():
        smalls = working[working['revenue'] < SMALL_THRESHOLD]
        if not smalls.empty:
            top3.iloc[2] = smalls.sort_values("similarity", ascending=False).iloc[0]

    # Label studio_size and ensure int year
    top3['studio_size'] = top3['revenue'].apply(
        lambda r: 'Small' if r < SMALL_THRESHOLD else 'Big'
    )
    top3["release_year"] = top3["release_year"].astype(int)

    # Return without revenue column
    return top3[[
        "original_title", "overview", "similarity", "release_year", "studio_size"
    ]]

# === Run & display ===
user_text = input("ðŸ’¬ What kind of movie are you in the mood for?\n")
show_example_matches(user_text)
print_genre_suggestions()
print_region_suggestions()

genre = input("\nðŸŽžï¸ Preferred genre? (Type 'Any' to skip):\n") or "Any"
region = input("\nðŸŒ Preferred region or country? (Type 'Any' to skip):\n") or "Any"
try:
    start_year = int(input("ðŸ“… Start year? (e.g. 1990):\n") or 1900)
    end_year   = int(input("ðŸ“… End year? (e.g. 2025):\n") or 2025)
except ValueError:
    start_year, end_year = 1900, 2025

recommendations = recommend_movies(user_text, genre, start_year, end_year, movie_region=region)
display(recommendations)

"""# Plotting using PCA Plot"""

from sklearn.decomposition import PCA
import plotly.express as px

# 1. Compute 3D PCA
pca3 = PCA(n_components=3, random_state=42)
X3 = pca3.fit_transform(X.toarray() if hasattr(X, "toarray") else X)

# 2. Build a DataFrame for plotting
df_plot = pd.DataFrame({
    "PC1": X3[:, 0],
    "PC2": X3[:, 1],
    "PC3": X3[:, 2],
    "cluster": df["cluster"].astype(str),
    "title": df["original_title"]
})

# 3. Plot
fig = px.scatter_3d(
    df_plot,
    x="PC1", y="PC2", z="PC3",
    color="cluster",
    hover_name="title",
    title="3D PCA of Movieâ€‘Tone Clusters"
)
fig.update_traces(marker=dict(size=4, opacity=0.7))
fig.show()

"""# Solution for df.size below:"""

# df.size - Returns the Total amount of Rows x Columns = 81,566
df.size

print(df.shape)
print("\nDf.shape separates both, so there are only 4,798 rows after preprocessing")
counts = df['tone'].value_counts()
print(counts)

terms = vectorizer.get_feature_names_out()


for i, comp in enumerate(pca3.components_, start=1):
    # pair each term with its loading on PC i
    term_weights = list(zip(terms, comp))
    # sort descending for positive, ascending for negative
    term_weights.sort(key=lambda x: x[1], reverse=True)

    top_pos = term_weights[:10]
    top_neg = term_weights[-10:]

    print(f"PC{i} most POSITIVE words:")
    for term, weight in top_pos:
        print(f"  {term:20} {weight:.4f}")

    print(f"\nPC{i} most NEGATIVE words:")
    for term, weight in top_neg:
        print(f"  {term:20} {weight:.4f}")
    print("\n" + "-"*40 + "\n")

"""# **EXPLORATORY DATA ANALYSIS (EDA)**"""

def extract_genre_names(genre_str):
    genres = ast.literal_eval(genre_str)
    return [g['name'] for g in genres]

df['genre_list'] = df['genres'].apply(extract_genre_names)
all_genres = [g for genre_list in df['genre_list'] for g in genre_list]

genre_counts = pd.Series(all_genres).value_counts().head(15)

plt.figure(figsize=(10,5))
sns.barplot(x=genre_counts.values, y=genre_counts.index, palette="viridis")
plt.title("Top 15 Genres Distribution")
plt.show()

# --- Release Year Distribution ---
plt.figure(figsize=(10,4))
sns.histplot(df['release_date'], bins=50, kde=False)
plt.title("Distribution of Movie Release Dates")
plt.xticks(rotation=45)
plt.show()

# --- Tone Distribution ---
plt.figure(figsize=(6,4))
sns.countplot(x='tone', data=df, palette='coolwarm')
plt.title("Distribution of Movie Tone (VADER classified)")
plt.show()

# Word cloud of overviews
from wordcloud import WordCloud
wordcloud_text = " ".join(df['clean_overview'])
wc = WordCloud(width=800, height=400, background_color="black").generate(wordcloud_text)
plt.figure(figsize=(10,5))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words in Overviews")
plt.show()

# Genre frequency bar chart
# Release year histogram
# Tone distribution
# Optional word cloud

"""# **FEATURE IMPORTANCE (TF-IDF)**"""

# Show the top 15 most important words based on TFâ€‘IDF overall.

# Show top 15 TF-IDF features by average importance
feature_array = np.array(vectorizer.get_feature_names_out())
tfidf_means = np.asarray(X.mean(axis=0)).flatten()
top_idx = tfidf_means.argsort()[::-1][:15]

print("Top 15 Most Important Words (TF-IDF):")
for idx in top_idx:
    print(f"{feature_array[idx]}: {tfidf_means[idx]:.4f}")

# Plot
plt.figure(figsize=(10,4))
sns.barplot(x=tfidf_means[top_idx], y=feature_array[top_idx], palette="magma")
plt.title("Top 15 TF-IDF Words by Importance")
plt.show()

# Shows which words matter most across all movie overviews.

"""# **MODEL EVALUATION: TONE CLASSIFIER (CONFUSION MATRIX)**"""

from google.colab import files

files.download('tone_labels.csv')

import pandas as pd
from google.colab import files
import io
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import matplotlib.pyplot as plt

sample_size = 50  # number of movies to label

# Step 1: Check if labeled CSV exists
try:
    tone_eval = pd.read_csv("tone_labels.csv")
    print("Loaded existing tone_labels.csv")
except FileNotFoundError:
    # Create sample CSV for manual labeling
    sample_df = df[['original_title', 'overview', 'tone']].sample(sample_size, random_state=42)
    sample_df.rename(columns={'tone': 'predicted_tone'}, inplace=True)
    sample_df['true_tone'] = ""  # empty column for labels
    sample_df.to_csv("tone_labels.csv", index=False)
    print(f"Created tone_labels.csv with {sample_size} movies.")
    print("Please download, fill in the 'true_tone' column (positive/neutral/negative), then upload the file back here.")

    # Trigger file download
    files.download("tone_labels.csv")

    # Stop execution to let user label
    raise SystemExit("Label the file and then re-run this cell after uploading.")

# Step 2: Upload labeled CSV
print("Please upload your labeled 'tone_labels.csv' file:")
uploaded = files.upload()

# Load uploaded CSV - Get the actual filename from the uploaded dictionary
uploaded_filename = list(uploaded.keys())[0]
tone_eval = pd.read_csv(io.BytesIO(uploaded[uploaded_filename]))


# Step 3: Verify all labels are filled
if tone_eval['true_tone'].astype(str).str.strip().eq("").any():
    print("âš ï¸ Some 'true_tone' labels are empty. Please fill them all before evaluation.")
else:
    # Merge with main df to align predictions with true labels
    # Use the original 'tone' column from df as the predicted tone for comparison
    eval_df = df.merge(tone_eval[['original_title', 'true_tone']], on='original_title', how='inner')
    eval_df = eval_df.rename(columns={'tone': 'predicted_tone'})


    y_true = eval_df['true_tone'].str.lower()
    y_pred = eval_df['predicted_tone'].str.lower()

    labels = ['positive', 'neutral', 'negative']

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix for VADER Tone Classification")
    plt.show()

    # Classification Report
    print(classification_report(y_true, y_pred))

"""# **ROC/AUC + METRICS TABLE**"""

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score, roc_curve
import itertools

try:
    # Binarize labels for ROC
    y_true_bin = label_binarize(y_true, classes=['positive','neutral','negative'])
    y_pred_bin = label_binarize(y_pred, classes=['positive','neutral','negative'])

    # ROC-AUC
    auc_score = roc_auc_score(y_true_bin, y_pred_bin, average="macro")
    print(f"Macro ROC-AUC: {auc_score:.3f}")

    # Plot ROC curve for each class
    fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_pred_bin.ravel())
    plt.figure(figsize=(6,4))
    plt.plot(fpr, tpr, label=f"Macro ROC (AUC={auc_score:.2f})")
    plt.plot([0,1],[0,1],'--',color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve for Tone Classification")
    plt.legend()
    plt.show()

except NameError:
    print("ROC skipped (need labeled data first).")

# Gives:
# ROC-AUC score
# ROC curve visualization